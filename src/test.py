
import tokenize
from resources.token_types import TOKEN_TYPES
from enum import Enum



class Modification(Enum):
    RENAME = 1
    DELETE = 2
  
FILE_NAME = 'src/example_file.py'


def get_token_list(file):
    """
    This function returns a list of tokens generated by python's tokenize module

    Inputs: 
        file: str, the file path of the python file to read
    
    Returns:
        tokens_list: list of tokenize.Token objects
    """
    with tokenize.open(file) as f:
        tokens = tokenize.generate_tokens(f.readline)
        tokens_list = list(tokens)

    return tokens_list
    


def find_lines_to_change(tokens_list, type_condition, text_condition):
    """
    Given a token type to change, this function returns a list of integers,
    which are the lines containing desired tokens to change.

    Inputs:
        tokens_list: list of tokenize.Token objects
        type_condition: int, which tokenize.exact_type is being searched for?
                        see resources/token_types.py
                        src: https://github.com/python/cpython/blob/3.13/Lib/token.py
        text_condition: str, the text we are looking for paired with type_condition,
                        this won't always be needed for some exact_types like 'PLUSEQUAL',
                        but lets us look up var names like type_condition: 'NAME', 
                        and 'my_var' for text_condition
    
    Returns:
        lines_to_change: list of ints, which lines have the token for desired change
    """
    lines_to_change = []

    for index, token in enumerate(tokens_list):
        if token.exact_type == type_condition and token.string == text_condition:
            if token.start[0] in lines_to_change:
                continue
            else: 
                lines_to_change.append(token.start[0])
        else:
            continue

    return lines_to_change


def create_line_dict(tokens_list):
    line_dict = {}

    for index, token in enumerate(tokens_list):
        if token.start[0] in line_dict.keys():
            line_dict[token.start[0]].append(index)
        else:
            line_dict[token.start[0]] = [index]

    return line_dict






def process_modifications(tokens_list, modifications_list):
    
    for mod in modifications_list:
        
        line = mod[0]['line']
        mod_type = mod[0]['type']
        print(line)
        
        not_processed = True
        
        for index, token in enumerate(tokens_list):
            if token.start[0] < line:
                continue
            if token.start[0] > line:
                break 
            
            
            print("We're at the correct token")
            print(token)
       




def replace_tokens(lines_to_change, tokens_list, line_dict, type_condition, text_condition, replacement_text):

    for line in lines_to_change:
        tokens_list = replace_line(tokens_list, line_dict[line], type_condition, text_condition, replacement_text)

    return(tokens_list)


def replace_line(tokens_list, token_indexes, type_condition, text_condition, replacement_text):

    num_replacements = 0
    length_diff = len(text_condition) - len(replacement_text) 

    for index in token_indexes:

        token = tokens_list[index]

        if token.exact_type == type_condition and token.string == text_condition:
 
            num_replacements += 1
                
            new_token = tokenize.TokenInfo(
                type=token.type,
                string=replacement_text,
                start= (token.start[0], token.start[1] - (length_diff * (num_replacements - 1))),
                end= (token.end[0], token.end[1] - (length_diff * num_replacements)),
                line=token.line
            )
            tokens_list[index] = new_token

        elif num_replacements > 0:
            new_token = tokenize.TokenInfo(
                type=token.type,
                string=token.string,
                start= (token.start[0], token.start[1] - (length_diff * num_replacements)),
                end= (token.end[0], token.end[1] - (length_diff * num_replacements)),
                line=token.line
            )
            tokens_list[index] = new_token

    return tokens_list


def main():
    
    
    tl = get_token_list(FILE_NAME)
    
    for i in tl:
        print(i)
    

    ld = create_line_dict(tl)

    print(ld)

    ml = find_lines_to_change(tl, TOKEN_TYPES['NAME'], "my_var") #, Modification.RENAME
    
    #for mod in ml:
    #   print(mod)

    tl = replace_tokens(ml, tl, ld, TOKEN_TYPES['NAME'], "my_var", "i")

    for t in tl:
        print(t)
    
    # Untokenize the tokens back to code
    reconstructed_code = tokenize.untokenize(tl)

    # Print the reconstructed code
    print("\nReconstructed Code:")
    print(reconstructed_code)

    
    
if __name__ == "__main__":
    main()